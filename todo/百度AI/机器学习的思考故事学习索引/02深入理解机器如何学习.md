<!-- TOC -->

- [深入理解机器如何学习](#深入理解机器如何学习)
    - [术语概念](#术语概念)
    - [前言](#前言)
    - [更多的优化目标](#更多的优化目标)
    - [更多寻解算法](#更多寻解算法)
    - [小结](#小结)

<!-- /TOC -->

# 深入理解机器如何学习


- 机器如何学习
    - 假设空间：比如线性关系
    - 优化目标：比如均方误差
    - 寻解算法：比如解微分方程
- 

## 术语概念

- 阶跃函数
- 均方误差
- 绝对值误差
- 梯度下降
- 最大似然 https://zhuanlan.zhihu.com/p/26614750
- 贝叶斯原理
- 离散函数

## 前言

答案 = 题目*知识

y = x*w

- 假设空间
    - 线性回归：牛顿第二定律（实数集）
    - 感知机：线性回归，分类场景，阶跃函数（0||1）
    - 逻辑回归：概率场景(0~1的值)



- 为什么这么多假设

    一个假设吃天下，不行么？

    假设实际上就是对目标规律的认知，由于对目标规律的认知不同，所以不同的应用场景存在不同的目标规律，需要不同的假设空间。（/马哲@一切从实际出发，实事求是/）

    以实用为目标，从需求出发构建假设

- 科学是什么

    科学是一种认识世界的信仰

    (/马哲@科学是什么，哲学是什么/)

## 更多的优化目标

- 线性回归

    牛顿第二定律案例

    完全拟合已知数据是理想假设，所以优化目标为拟合误差的某种衡量指标，成为Loss

    - 均方误差

        均方误差更柔和，更容易更快速的感知到最低点。绝对误差是直接找到，但不容易感知到最低点，且不柔和。

    - 优化解决过程

        合理性：绝对误差
        
        易解性：均方误差

- 感知机

    - 二分类模型：True|False ：： Positive||Negative
    - 0/1误差

        0/1误差：Min(FP+FN)，很不好解


- 逻辑回归

    - 似然误差

        预测输出（概率值）与真实分类（0||1）不是在同一个维度上，不能直接套用均方误差

        最大似然、概率函数、似然误差：https://zhuanlan.zhihu.com/p/26614750

    - 概率函数

        p(h|d)

    - 优化解决过程
    
        合理性推导：贝叶斯原理

        易解性推导：+Log 求最小


    当你排除一切不可能的情况，剩下的，不管多难以置信，都是事实。-- 福尔摩斯（/马哲@实事求是/）



- 一个启示性的故事

    一位老人在乡村养病,但附近的小孩很喜欢在老人家附近玩耍,于是老人把孩子们都聚在一起,说谁的声音最大就有奖赏,并根据他们的吵闹程度给予不同的奖励,在孩子们都习惯有奖励的时候,慢慢的老人就不再给奖励,孩子们就想,你不给我钱,我为什么给你吵,之后就没有在老人家吵了.

    /马哲@外部与内部因素，表面现象与本质/

    孩子们原本吵闹是为了玩耍，后来被老人变相更改了吵闹的目的，让表面现象给迷惑了，丢失了吵闹的本质


    合理的优化目标，根据当下实际情况实现，才更有意义

## 更多寻解算法


- 线性代数

    大学里的线性代数，很需要改革，砍掉其中一半，保留基本的那一半，再结合实际应用场景，深入思考线性代数的思想（原理）与用途（方法论）。毕竟大部分人学过线性代数，在之后的人生中可能基本都不会再使用到，在这里就想到了，美国及西方部分教育，注重基础及实践结合，是很有道理的，他们并不是一味着为了学习而学习，为了竞赛而学习。工具科目容易快速上手学习、思想科目学个基础及思想入门，待研究生去深入研究。

    说到这里，预测10年后，中国教育需要改革成：小学缩短最多4~5年，初中与高中合并成中学，3~4年，大学2~3年，研究生2年，比起原来从7岁开始6+3+3+4+2，学到研究生时至少25岁，改革后可以从7岁开始5+4+3+2，学到研究生时21岁。21世纪的孩子都比较成熟，也容易早早进入社会，同样早早推动家庭的走向。人类年轻化是必然的，人类老年化也是必然的。

    时代是发展的，20年前适合的规则，在20年后不一定适合，需要与时俱进，物质的变化的累积，导致质变，质变促进社会行业的变革。

    /马哲@发展/

- 近朱者赤近墨者黑

    这种情况的才适合机器学习

    蝴蝶效应、股票系统、彩票系统不适合机器学习（混沌系统）

- 梯度下降

    随机取一个参数w，计算导函数，得到当前梯度（斜率），继续往下降的方向走一小步，逐步一直走

    - 可行性：凸函数，全局唯一最低点
    - 关键要素：方向、步长
    
    - 步长
        - 太短：优化太慢
        - 太长：可能错过最低点
        - 具体应用场景的适合步长，在具体应用场景中测试得到适合步长
        - 步长灵活，根据梯度灵活调整，一开始长，慢慢的越来越精细

- 感知机：PLA

    将离散的错误样本到分类界面的距离量化成连续的函数

- 逻辑回归：梯度下降优化

    参考PLA，每轮迭代只考虑部分样本，去计算当前的梯度，得到的每次梯度方向可能都会不一样，但整体最终的大方向是往低点的方向，速度快，但快接近最低点时，会来回徘徊（毕老师给了个很形象的比喻：奔跑的醉汉）



- 寻解算法

    是三要素中更偏数学设计，离应用相对更远的要素


## 小结

- 线性回归

    - 回归假设
    - 均方误差
    - 公式解

    现实中，存在较多常见的类线性关系

- 感知机
    - 分类假设
    - 0/1误差
    - 迭代优化

- 逻辑回归
    - 概率假设
    - 似然误差
    - 梯度下降

> 毕老师，更多的是分享机器学习的认知、思想，而不是分享具体的模型算法知识，这些认知和思想更像是大楼的基石和架构，只有基石和架构牢固，大楼的建造更快速更稳。

----
视频：https://aistudio.baidu.com/aistudio/education/lessonvideo/280029




